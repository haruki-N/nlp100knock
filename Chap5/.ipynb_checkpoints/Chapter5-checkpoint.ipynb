{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第５章：係受け解析\n",
    "ファイルai.ja.zipに収められている人工知能に関するテキストを係受け解析して,結果をai.ja.txt.parsedに保存せよ\n",
    "\n",
    "まずはファイルのダウンロードをwgetコマンドを使ってnotebookに直接ダウンロードする.\n",
    "wgetコマンド実行時に -nc オプションをつけることで既にダウンロードされている場合は重複ダウンロードを回避してくれる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-11 13:53:08--  https://nlp100.github.io/data/ai.ja.zip\n",
      "nlp100.github.io (nlp100.github.io) をDNSに問いあわせています... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
      "nlp100.github.io (nlp100.github.io)|185.199.110.153|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 17516 (17K) [application/zip]\n",
      "`ai.ja.zip' に保存中\n",
      "\n",
      "ai.ja.zip           100%[===================>]  17.11K  --.-KB/s 時間 0.001s     \n",
      "\n",
      "2020-12-11 13:53:08 (13.6 MB/s) - `ai.ja.zip' へ保存完了 [17516/17516]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://nlp100.github.io/data/ai.ja.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wgetコマンドによるzipファイルのダウンロードができたので,続いてunzipコマンドを使ってファイルを解凍する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ai.ja.zip\n",
      "  inflating: ai.ja.txt               \n",
      "  inflating: readme.ai.ja.md         \n"
     ]
    }
   ],
   "source": [
    "!unzip ai.ja.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\r\n",
      "\r\n",
      "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\r\n",
      "\r\n",
      "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ai.ja.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただし,このままではテキストファイルが文区切りになっていないので,文区切りのファイルを作成し,これを対象に係受け解析する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ai.ja.txt') as fi, open('ai.ja.txt.splited', 'w') as output_fi:\n",
    "    for line in fi:\n",
    "        output_fi.write(line.replace('。', '。\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "使用するファイルの準備ができたので,続いてCaboChaをインストールする.CaboChaを使うには依存ライブラリであるCRF++もインストールする必要があるっぽい.\n",
    "> ### CRFとは\n",
    "CRF(Conditional Random Field)は系列ラベリング(各単語に品詞を割り当てる的な問題)を解くためのもの.系列ラベリングは分類問題として解くことができるので,各々の品詞への割り当て確率を計算することで解ける.これは**「識別モデル」** を使うことで確率を算出できる.CRFではこの確率算出の時に,与えた入力文に適した品詞を割り当てられるように入力全体での整合性を取ろうとする.この整合性のために「構造学習」を行うのがCRFの特徴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    太郎は---------D\r\n",
      "      花子が-D     |\r\n",
      "    読んでいる-D   |\r\n",
      "            本を---D\r\n",
      "            次郎に-D\r\n",
      "              渡した\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "# 実際にCaboChaを使ってみる\n",
    "!echo \"太郎は花子が読んでいる本を次郎に渡した\" | cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 5D 0/1 -0.742128\r\n",
      "太郎\t名詞,固有名詞,人名,名,*,*,太郎,タロウ,タロー\r\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\r\n",
      "* 1 2D 0/1 1.700175\r\n",
      "花子\t名詞,固有名詞,人名,名,*,*,花子,ハナコ,ハナコ\r\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\r\n",
      "* 2 3D 0/2 1.825021\r\n",
      "読ん\t動詞,自立,*,*,五段・マ行,連用タ接続,読む,ヨン,ヨン\r\n",
      "で\t助詞,接続助詞,*,*,*,*,で,デ,デ\r\n",
      "いる\t動詞,非自立,*,*,一段,基本形,いる,イル,イル\r\n",
      "* 3 5D 0/1 -0.742128\r\n",
      "本\t名詞,一般,*,*,*,*,本,ホン,ホン\r\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\r\n",
      "* 4 5D 1/2 -0.742128\r\n",
      "次\t名詞,一般,*,*,*,*,次,ツギ,ツギ\r\n",
      "郎\t名詞,一般,*,*,*,*,郎,ロウ,ロー\r\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\r\n",
      "* 5 -1D 0/1 0.000000\r\n",
      "渡し\t動詞,自立,*,*,五段・サ行,連用形,渡す,ワタシ,ワタシ\r\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "# cabochaコマンドはオプションに -f1 を渡すと計算機に処理しやすいフォーマットで出力してくれる\n",
    "!echo \"太郎は花子が読んでいる本を次郎に渡した\" | cabocha -f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai.ja.txtのファイルをCaboChaで係受け解析する\n",
    "!cat ai.ja.txt.splited| cabocha -f1 > ai.ja.txt.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 1/1 0.000000\r\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "EOS\r\n",
      "EOS\r\n",
      "* 0 17D 1/1 -1.776924\r\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "* 1 17D 2/3 -1.776924\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n"
     ]
    }
   ],
   "source": [
    "!head ai.ja.txt.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "> 形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    # __repr__:特殊メソッド.jupyter notebookなどではprintを使わず、変数名を指定するだけで出力結果を得ようとするときに呼び出される\n",
    "    # 関数repr():オブジェクト情報を返してくれる.ただの文字列なのかそうでないのか表面上わかりづらい時などに便利.(主にデバック用...?)\n",
    "    def __repr__(self):\n",
    "        return f\"Morph(surface={repr(self.surface)}, base={repr(self.base)}, pos={repr(self.pos)}, pos1={repr(self.pos1)})\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Morph(surface='サーフェス', base='ベース', pos='ポス', pos1='ポスいち')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Morphクラス使用例\n",
    "m = Morph(\"サーフェス\", \"ベース\", \"ポス\", \"ポスいち\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の出力を見ると特にprintなどを使っていないのにもかかわらずmの情報がプリントされている <- `__repr__` のおかげ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 1/1 0.000000\r\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "EOS\r\n",
      "EOS\r\n",
      "* 0 17D 1/1 -1.776924\r\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "* 1 17D 2/3 -1.776924\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n",
      "じん\t名詞,一般,*,*,*,*,じん,ジン,ジン\r\n",
      "こうち\t名詞,一般,*,*,*,*,こうち,コウチ,コーチ\r\n",
      "のう\t助詞,終助詞,*,*,*,*,のう,ノウ,ノー\r\n",
      "、\t記号,読点,*,*,*,*,、,、,、\r\n",
      "、\t記号,読点,*,*,*,*,、,、,、\r\n",
      "* 2 3D 0/0 0.758984\r\n",
      "AI\t名詞,一般,*,*,*,*,*\r\n",
      "* 3 17D 1/5 -1.776924\r\n",
      "〈\t記号,括弧開,*,*,*,*,〈,〈,〈\r\n",
      "エーアイ\t名詞,固有名詞,一般,*,*,*,*\r\n",
      "〉\t記号,括弧閉,*,*,*,*,〉,〉,〉\r\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\r\n",
      "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\r\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\r\n",
      "、\t記号,読点,*,*,*,*,、,、,、\r\n",
      "* 4 5D 2/2 1.035972\r\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\r\n",
      "『\t記号,括弧開,*,*,*,*,『,『,『\r\n",
      "計算\t名詞,サ変接続,*,*,*,*,計算,ケイサン,ケイサン\r\n",
      "* 5 9D 0/3 1.243687\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\r\n",
      "』\t記号,括弧閉,*,*,*,*,』,』,』\r\n",
      "という\t助詞,格助詞,連語,*,*,*,という,トイウ,トユウ\r\n",
      "* 6 9D 0/1 0.691934\r\n",
      "概念\t名詞,一般,*,*,*,*,概念,ガイネン,ガイネン\r\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\r\n",
      "* 7 8D 1/1 1.048596\r\n",
      "『\t記号,括弧開,*,*,*,*,『,『,『\r\n",
      "コンピュータ\t名詞,一般,*,*,*,*,コンピュータ,コンピュータ,コンピュータ\r\n",
      "* 8 9D 0/3 1.540775\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\r\n",
      "』\t記号,括弧閉,*,*,*,*,』,』,』\r\n",
      "という\t助詞,格助詞,連語,*,*,*,という,トイウ,トユウ\r\n",
      "* 9 10D 0/1 2.129047\r\n",
      "道具\t名詞,一般,*,*,*,*,道具,ドウグ,ドーグ\r\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\r\n",
      "* 10 12D 0/1 1.363632\r\n",
      "用い\t動詞,自立,*,*,一段,連用形,用いる,モチイ,モチイ\r\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\r\n",
      "* 11 12D 1/3 2.086987\r\n",
      "『\t記号,括弧開,*,*,*,*,『,『,『\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "』\t記号,括弧閉,*,*,*,*,』,』,』\r\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\r\n",
      "* 12 13D 1/1 0.529353\r\n",
      "研究\t名詞,サ変接続,*,*,*,*,研究,ケンキュウ,ケンキュー\r\n",
      "する\t動詞,自立,*,*,サ変・スル,基本形,する,スル,スル\r\n",
      "* 13 14D 2/2 0.314025\r\n",
      "計算\t名詞,サ変接続,*,*,*,*,計算,ケイサン,ケイサン\r\n",
      "機\t名詞,接尾,一般,*,*,*,機,キ,キ\r\n",
      "科学\t名詞,一般,*,*,*,*,科学,カガク,カガク\r\n",
      "* 14 15D 0/2 4.311133\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\r\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\r\n",
      "* 15 16D 1/3 2.275064\r\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\r\n",
      "分野\t名詞,一般,*,*,*,*,分野,ブンヤ,ブンヤ\r\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\r\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\r\n",
      "* 16 17D 0/0 -1.776924\r\n",
      "指す\t動詞,自立,*,*,五段・サ行,基本形,指す,サス,サス\r\n",
      "* 17 -1D 0/0 0.000000\r\n",
      "語\t名詞,一般,*,*,*,*,語,カタリ,カタリ\r\n",
      "。\t記号,句点,*,*,*,*,。,。,。\r\n",
      "EOS\r\n",
      "* 0 2D 1/2 0.552409\r\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\r\n",
      "言語\t名詞,一般,*,*,*,*,言語,ゲンゴ,ゲンゴ\r\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\r\n",
      "* 1 2D 0/1 2.217661\r\n",
      "理解\t名詞,サ変接続,*,*,*,*,理解,リカイ,リカイ\r\n",
      "や\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\r\n",
      "* 2 3D 0/0 0.257647\r\n",
      "推論\t名詞,サ変接続,*,*,*,*,推論,スイロン,スイロン\r\n",
      "、\t記号,読点,*,*,*,*,、,、,、\r\n",
      "* 3 4D 1/3 2.083214\r\n",
      "問題\t名詞,ナイ形容詞語幹,*,*,*,*,問題,モンダイ,モンダイ\r\n",
      "解決\t名詞,サ変接続,*,*,*,*,解決,カイケツ,カイケツ\r\n",
      "など\t助詞,副助詞,*,*,*,*,など,ナド,ナド\r\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\r\n",
      "* 4 6D 1/2 0.693399\r\n",
      "知的\t名詞,一般,*,*,*,*,知的,チテキ,チテキ\r\n",
      "行動\t名詞,サ変接続,*,*,*,*,行動,コウドウ,コードー\r\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\r\n",
      "* 5 6D 0/1 2.190897\r\n",
      "人間\t名詞,一般,*,*,*,*,人間,ニンゲン,ニンゲン\r\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\r\n",
      "* 6 8D 0/1 1.505879\r\n",
      "代わっ\t動詞,自立,*,*,五段・ラ行,連用タ接続,代わる,カワッ,カワッ\r\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\r\n",
      "* 7 8D 0/1 2.525808\r\n",
      "コンピューター\t名詞,一般,*,*,*,*,コンピューター,コンピューター,コンピューター\r\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\r\n",
      "* 8 9D 0/1 3.383079\r\n",
      "行わ\t動詞,自立,*,*,五段・ワ行促音便,未然形,行う,オコナワ,オコナワ\r\n",
      "せる\t動詞,接尾,*,*,一段,基本形,せる,セル,セル\r\n",
      "* 9 16D 3/3 1.275360\r\n",
      "技術\t名詞,一般,*,*,*,*,技術,ギジュツ,ギジュツ\r\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\r\n",
      "、\t記号,読点,*,*,*,*,、,、,、\r\n",
      "または\t接続詞,*,*,*,*,*,または,マタハ,マタワ\r\n",
      "、\t記号,読点,*,*,*,*,、,、,、\r\n",
      "* 10 11D 2/2 0.253432\r\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\r\n",
      "計算\t名詞,サ変接続,*,*,*,*,計算,ケイサン,ケイサン\r\n",
      "機\t名詞,接尾,一般,*,*,*,機,キ,キ\r\n",
      "* 11 13D 1/3 1.353133\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n",
      "コンピュータ\t名詞,一般,*,*,*,*,コンピュータ,コンピュータ,コンピュータ\r\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\r\n",
      "による\t助詞,格助詞,連語,*,*,*,による,ニヨル,ニヨル\r\n",
      "* 12 13D 0/1 1.098217\r\n",
      "知的\t名詞,形容動詞語幹,*,*,*,*,知的,チテキ,チテキ\r\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\r\n",
      "* 13 15D 1/2 0.917915\r\n",
      "情報処理\t名詞,一般,*,*,*,*,情報処理,ジョウホウショリ,ジョーホーショリ\r\n",
      "システム\t名詞,一般,*,*,*,*,システム,システム,システム\r\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\r\n",
      "* 14 15D 0/1 1.201759\r\n",
      "設計\t名詞,サ変接続,*,*,*,*,設計,セッケイ,セッケイ\r\n",
      "や\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\r\n",
      "* 15 16D 0/1 3.229985\r\n",
      "実現\t名詞,サ変接続,*,*,*,*,実現,ジツゲン,ジツゲン\r\n",
      "に関する\t助詞,格助詞,連語,*,*,*,に関する,ニカンスル,ニカンスル\r\n",
      "* 16 17D 1/4 1.275360\r\n",
      "研究\t名詞,サ変接続,*,*,*,*,研究,ケンキュウ,ケンキュー\r\n",
      "分野\t名詞,一般,*,*,*,*,分野,ブンヤ,ブンヤ\r\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\r\n",
      "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\r\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\r\n",
      "* 17 -1D 0/1 0.000000\r\n",
      "さ\t動詞,自立,*,*,サ変・スル,未然レル接続,する,サ,サ\r\n",
      "れる\t動詞,接尾,*,*,一段,基本形,れる,レル,レル\r\n",
      "。\t記号,句点,*,*,*,*,。,。,。\r\n",
      "EOS\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "# 冒頭の説明文だけを取り出して係受け解析し,別のファイルとして保存\n",
    "!head -n5 ai.ja.txt.splited | cabocha -f1 > ai.ja.txt.parsed.head\n",
    "!cat ai.ja.txt.parsed.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CaboChaによる係受け解析結果について\n",
    "出力フォーマットは以下の通り\n",
    "> (1行目)* 文節番号 係り先の文節番号(係り先なし:-1) 主辞の形態素番号/機能語の形態素番号 係り関係のスコア(大きい方が係りやすい)\n",
    "\n",
    "> (2行目)表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[Morph(surface='人工', base='人工', pos='名詞', pos1='一般'),\n",
      " Morph(surface='知能', base='知能', pos='名詞', pos1='一般'),\n",
      " Morph(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='じん', base='じん', pos='名詞', pos1='一般'),\n",
      " Morph(surface='こうち', base='こうち', pos='名詞', pos1='一般'),\n",
      " Morph(surface='のう', base='のう', pos='助詞', pos1='終助詞'),\n",
      " Morph(surface='、', base='、', pos='記号', pos1='読点'),\n",
      " Morph(surface='、', base='、', pos='記号', pos1='読点'),\n",
      " Morph(surface='AI', base='*', pos='名詞', pos1='一般'),\n",
      " Morph(surface='〈', base='〈', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='エーアイ', base='*', pos='名詞', pos1='固有名詞'),\n",
      " Morph(surface='〉', base='〉', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='と', base='と', pos='助詞', pos1='格助詞'),\n",
      " Morph(surface='は', base='は', pos='助詞', pos1='係助詞'),\n",
      " Morph(surface='、', base='、', pos='記号', pos1='読点'),\n",
      " Morph(surface='「', base='「', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='『', base='『', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='計算', base='計算', pos='名詞', pos1='サ変接続'),\n",
      " Morph(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='』', base='』', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='という', base='という', pos='助詞', pos1='格助詞'),\n",
      " Morph(surface='概念', base='概念', pos='名詞', pos1='一般'),\n",
      " Morph(surface='と', base='と', pos='助詞', pos1='並立助詞'),\n",
      " Morph(surface='『', base='『', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='コンピュータ', base='コンピュータ', pos='名詞', pos1='一般'),\n",
      " Morph(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='』', base='』', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='という', base='という', pos='助詞', pos1='格助詞'),\n",
      " Morph(surface='道具', base='道具', pos='名詞', pos1='一般'),\n",
      " Morph(surface='を', base='を', pos='助詞', pos1='格助詞'),\n",
      " Morph(surface='用い', base='用いる', pos='動詞', pos1='自立'),\n",
      " Morph(surface='て', base='て', pos='助詞', pos1='接続助詞'),\n",
      " Morph(surface='『', base='『', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='知能', base='知能', pos='名詞', pos1='一般'),\n",
      " Morph(surface='』', base='』', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='を', base='を', pos='助詞', pos1='格助詞'),\n",
      " Morph(surface='研究', base='研究', pos='名詞', pos1='サ変接続'),\n",
      " Morph(surface='する', base='する', pos='動詞', pos1='自立'),\n",
      " Morph(surface='計算', base='計算', pos='名詞', pos1='サ変接続'),\n",
      " Morph(surface='機', base='機', pos='名詞', pos1='接尾'),\n",
      " Morph(surface='科学', base='科学', pos='名詞', pos1='一般'),\n",
      " Morph(surface='（', base='（', pos='記号', pos1='括弧開'),\n",
      " Morph(surface='）', base='）', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='の', base='の', pos='助詞', pos1='連体化'),\n",
      " Morph(surface='一', base='一', pos='名詞', pos1='数'),\n",
      " Morph(surface='分野', base='分野', pos='名詞', pos1='一般'),\n",
      " Morph(surface='」', base='」', pos='記号', pos1='括弧閉'),\n",
      " Morph(surface='を', base='を', pos='助詞', pos1='格助詞'),\n",
      " Morph(surface='指す', base='指す', pos='動詞', pos1='自立'),\n",
      " Morph(surface='語', base='語', pos='名詞', pos1='一般'),\n",
      " Morph(surface='。', base='。', pos='記号', pos1='句点')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "\n",
    "# テキスト1行分を受け取って各形態素を辞書形式にして返す変数\n",
    "# 第４章でやった形態素の各要素を取得する方法と同じ\n",
    "def parse_morph_line(line):\n",
    "    p = line.find('\\t')   # タブ文字のインデックス番号を返す\n",
    "    assert p != -1, 'The line does not contain a TAB character.'\n",
    "    contents = line[p+1:].split(',')\n",
    "    assert len(contents) >= 7, f'The line sholud have more than 7 contents separated by comma.line={line}'\n",
    "    # m = Morph\n",
    "    return {'surface':line[:p], 'base':contents[6], 'pos':contents[0], 'pos1':contents[1]}\n",
    "\n",
    "# 入力テキスト(cabocha解析結果)に対して目的の出力(Morphオブジェクトのリスト)を得るための関数\n",
    "def cabocha_reader(text):\n",
    "    morphs = []\n",
    "    for line in text:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line == 'EOS' and morphs:   # EOSを見つける&morphsが空じゃない = 1行分解析完了\n",
    "            yield morphs\n",
    "            morphs = []   # リセット\n",
    "        elif '\\t' in line:\n",
    "            morph = parse_morph_line(line)\n",
    "            morphs.append(Morph(morph['surface'], morph['base'], morph['pos'], morph['pos1']))\n",
    "            \n",
    "with open('ai.ja.txt.parsed.head') as f:\n",
    "    morph = list(cabocha_reader(f))\n",
    "    \n",
    "print(len(morph))\n",
    "pprint(morph[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    ">40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 1/1 0.000000\r\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "EOS\r\n",
      "EOS\r\n",
      "* 0 17D 1/1 -1.776924\r\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\r\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\r\n",
      "* 1 17D 2/3 -1.776924\r\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 ai.ja.txt.parsed.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk(morphs=[Morph(surface='人工', base='人工', pos='名詞', pos1='一般'), Morph(surface='知能', base='知能', pos='名詞', pos1='一般')], dst=17, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='（', base='（', pos='記号', pos1='括弧開'), Morph(surface='じん', base='じん', pos='名詞', pos1='一般'), Morph(surface='こうち', base='こうち', pos='名詞', pos1='一般'), Morph(surface='のう', base='のう', pos='助詞', pos1='終助詞'), Morph(surface='、', base='、', pos='記号', pos1='読点'), Morph(surface='、', base='、', pos='記号', pos1='読点')], dst=17, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='AI', base='*', pos='名詞', pos1='一般')], dst=3, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='〈', base='〈', pos='記号', pos1='括弧開'), Morph(surface='エーアイ', base='*', pos='名詞', pos1='固有名詞'), Morph(surface='〉', base='〉', pos='記号', pos1='括弧閉'), Morph(surface='）', base='）', pos='記号', pos1='括弧閉'), Morph(surface='と', base='と', pos='助詞', pos1='格助詞'), Morph(surface='は', base='は', pos='助詞', pos1='係助詞'), Morph(surface='、', base='、', pos='記号', pos1='読点')], dst=17, srcs=[2]),\n",
      " Chunk(morphs=[Morph(surface='「', base='「', pos='記号', pos1='括弧開'), Morph(surface='『', base='『', pos='記号', pos1='括弧開'), Morph(surface='計算', base='計算', pos='名詞', pos1='サ変接続')], dst=5, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='（', base='（', pos='記号', pos1='括弧開'), Morph(surface='）', base='）', pos='記号', pos1='括弧閉'), Morph(surface='』', base='』', pos='記号', pos1='括弧閉'), Morph(surface='という', base='という', pos='助詞', pos1='格助詞')], dst=9, srcs=[4]),\n",
      " Chunk(morphs=[Morph(surface='概念', base='概念', pos='名詞', pos1='一般'), Morph(surface='と', base='と', pos='助詞', pos1='並立助詞')], dst=9, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='『', base='『', pos='記号', pos1='括弧開'), Morph(surface='コンピュータ', base='コンピュータ', pos='名詞', pos1='一般')], dst=8, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='（', base='（', pos='記号', pos1='括弧開'), Morph(surface='）', base='）', pos='記号', pos1='括弧閉'), Morph(surface='』', base='』', pos='記号', pos1='括弧閉'), Morph(surface='という', base='という', pos='助詞', pos1='格助詞')], dst=9, srcs=[7]),\n",
      " Chunk(morphs=[Morph(surface='道具', base='道具', pos='名詞', pos1='一般'), Morph(surface='を', base='を', pos='助詞', pos1='格助詞')], dst=10, srcs=[5, 6, 8]),\n",
      " Chunk(morphs=[Morph(surface='用い', base='用いる', pos='動詞', pos1='自立'), Morph(surface='て', base='て', pos='助詞', pos1='接続助詞')], dst=12, srcs=[9]),\n",
      " Chunk(morphs=[Morph(surface='『', base='『', pos='記号', pos1='括弧開'), Morph(surface='知能', base='知能', pos='名詞', pos1='一般'), Morph(surface='』', base='』', pos='記号', pos1='括弧閉'), Morph(surface='を', base='を', pos='助詞', pos1='格助詞')], dst=12, srcs=[]),\n",
      " Chunk(morphs=[Morph(surface='研究', base='研究', pos='名詞', pos1='サ変接続'), Morph(surface='する', base='する', pos='動詞', pos1='自立')], dst=13, srcs=[10, 11]),\n",
      " Chunk(morphs=[Morph(surface='計算', base='計算', pos='名詞', pos1='サ変接続'), Morph(surface='機', base='機', pos='名詞', pos1='接尾'), Morph(surface='科学', base='科学', pos='名詞', pos1='一般')], dst=14, srcs=[12]),\n",
      " Chunk(morphs=[Morph(surface='（', base='（', pos='記号', pos1='括弧開'), Morph(surface='）', base='）', pos='記号', pos1='括弧閉'), Morph(surface='の', base='の', pos='助詞', pos1='連体化')], dst=15, srcs=[13]),\n",
      " Chunk(morphs=[Morph(surface='一', base='一', pos='名詞', pos1='数'), Morph(surface='分野', base='分野', pos='名詞', pos1='一般'), Morph(surface='」', base='」', pos='記号', pos1='括弧閉'), Morph(surface='を', base='を', pos='助詞', pos1='格助詞')], dst=16, srcs=[14]),\n",
      " Chunk(morphs=[Morph(surface='指す', base='指す', pos='動詞', pos1='自立')], dst=17, srcs=[15]),\n",
      " Chunk(morphs=[Morph(surface='語', base='語', pos='名詞', pos1='一般'), Morph(surface='。', base='。', pos='記号', pos1='句点')], dst=-1, srcs=[0, 1, 3, 16])]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Chunk(morphs={repr(self.morphs)}, dst={repr(self.dst)}, srcs={repr(self.srcs)})\"\n",
    "\n",
    "# 再掲\n",
    "# (1行目)* 文節番号 係り先の文節番号(係り先なし:-1) 主辞の形態素番号/機能語の形態素番号 係り関係のスコア(大きい方が係りやすい)\n",
    "\n",
    "def parse_chunk_line(line):\n",
    "    p = line.find('*')\n",
    "    assert p != -1, 'The line should contain * mark in it'\n",
    "    contents = line[p+2:].split(' ')\n",
    "    assert len(contents) >= 4, f'The line should have 4 contents in it.{line}'\n",
    "    return {'index':contents[0], 'dst':contents[1], 'srcs':contents[2]}\n",
    "    \n",
    "def chunk_reader(text):\n",
    "    dsts = defaultdict(int)   # 係先を格納\n",
    "    srcs = defaultdict(list)   # 係元を格納\n",
    "    morphs = defaultdict(list)   # 各形態素を格納\n",
    "    \n",
    "    for line in text:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line == 'EOS' and morphs:\n",
    "            yield [Chunk(morphs[index], dsts[index], srcs[index]) for index in range(len(dsts))]\n",
    "            # 諸々のリセット\n",
    "            dsts = defaultdict(int)\n",
    "            srcs = defaultdict(list)\n",
    "            morphs = defaultdict(list)\n",
    "\n",
    "        elif line.startswith('*'):\n",
    "            parsed = parse_chunk_line(line)\n",
    "            dst = int(parsed['dst'].rstrip('D'))   # 係先の文節番号\n",
    "            index = int(parsed['index'])   # 自身の文節番号\n",
    "            dsts[index] = dst   # 文節番号nの文節の係先をわかるようにしておく\n",
    "            if dst == -1:   # 係先がない場合\n",
    "                dsts[index] = -1\n",
    "            else:   # 係元のindex番号をリストに追加\n",
    "                srcs[dst].append(index)\n",
    "                \n",
    "        elif '\\t' in line:\n",
    "            morph = parse_morph_line(line)\n",
    "            morphs[index].append(Morph(morph['surface'], morph['base'], morph['pos'], morph['pos1']))\n",
    "            \n",
    "with open('ai.ja.txt.parsed.head') as f:\n",
    "    chunk = list(chunk_reader(f))\n",
    "    \n",
    "pprint(chunk[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    ">係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能 -> 係先なし\n",
      "\n",
      "EOS\n",
      "\n",
      "人工知能 -> 語\n",
      "じんこうちのう -> 語\n",
      "AI -> エーアイとは\n",
      "エーアイとは -> 語\n",
      "計算 -> という\n",
      "という -> 道具を\n",
      "概念と -> 道具を\n",
      "コンピュータ -> という\n",
      "という -> 道具を\n",
      "道具を -> 用いて\n",
      "用いて -> 研究する\n",
      "知能を -> 研究する\n",
      "研究する -> 計算機科学\n",
      "計算機科学 -> の\n",
      "の -> 一分野を\n",
      "一分野を -> 指す\n",
      "指す -> 語\n",
      "語 -> 係先なし\n",
      "\n",
      "EOS\n",
      "\n",
      "言語の -> 推論\n",
      "理解や -> 推論\n",
      "推論 -> 問題解決などの\n",
      "問題解決などの -> 知的行動を\n",
      "知的行動を -> 代わって\n",
      "人間に -> 代わって\n",
      "代わって -> 行わせる\n",
      "コンピューターに -> 行わせる\n",
      "行わせる -> 技術または\n",
      "技術または -> 研究分野とも\n",
      "計算機 -> コンピュータによる\n",
      "コンピュータによる -> 情報処理システムの\n",
      "知的な -> 情報処理システムの\n",
      "情報処理システムの -> 実現に関する\n",
      "設計や -> 実現に関する\n",
      "実現に関する -> 研究分野とも\n",
      "研究分野とも -> される\n",
      "される -> 係先なし\n",
      "\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 記号以外の形態素の表層形を取得し,それらをまとめて出力する\n",
    "def show_clause(morphs: list):\n",
    "    return [print(morph.surface, sep='', end='') for morph in morphs if morph.pos != '記号']\n",
    "\n",
    "with open('ai.ja.txt.parsed.head') as f:\n",
    "    chunks_list = list(chunk_reader(f))\n",
    "    \n",
    "    # 各EOS毎に処理を繰り返す\n",
    "    for chunks in chunks_list:  # 1文 in テキスト全体\n",
    "        for chunk in chunks:   # 1文節 in 1文\n",
    "            # 係元の文節を出力\n",
    "            show_clause(chunk.morphs)\n",
    "            \n",
    "            # 係先の文節を出力\n",
    "            if chunk.dst == -1:\n",
    "                print(' -> '+'係先なし')\n",
    "            else:\n",
    "                print(' -> ', end='')\n",
    "                show_clause(chunks[chunk.dst].morphs)\n",
    "                print('')\n",
    "        print('\\n'+'EOS'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    ">名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EOS\n",
      "\n",
      "道具を -> 用いて\n",
      "知能を -> 研究する\n",
      "一分野を -> 指す\n",
      "\n",
      "EOS\n",
      "\n",
      "知的行動を -> 代わって\n",
      "人間に -> 代わって\n",
      "コンピューターに -> 行わせる\n",
      "研究分野とも -> される\n",
      "\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('ai.ja.txt.parsed.head') as f:\n",
    "    chunks_list = list(chunk_reader(f))\n",
    "    \n",
    "    # 各EOS毎に処理を繰り返す\n",
    "    for chunks in chunks_list:\n",
    "        for chunk in chunks:\n",
    "            # 係元の文節に名詞が含まれているかどうかチェック\n",
    "            if any([morph for morph in chunk.morphs if morph.pos == '名詞']):\n",
    "                # 係先の文節に動詞が含まれているどうかチェック\n",
    "                if any([morph for morph in chunks[chunk.dst].morphs if morph.pos == '動詞']):\n",
    "                    show_clause(chunk.morphs)\n",
    "                    print(' -> ', end='')\n",
    "                    show_clause(chunks[chunk.dst].morphs)\n",
    "                    print('')\n",
    "\n",
    "        print('\\n'+'EOS'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化Permalink\n",
    ">与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphvizの使い方\n",
    "基本的な流れは以下の通り.\n",
    "1. グラフオブジェクトの生成\n",
    "> 無向グラフを描くGraphモジュールと有向グラフを描くDigraphモジュールをインポート.  \n",
    "`dg = Digraph(format='png')`みたいにしてオブジェクト生成.引数に指定したフォーマット形式で保存できる.\n",
    "2. ノードやエッジを追加\n",
    "> オブジェクトにメソッドでnodeやedgeを追加していく.  \n",
    "`dg.node('1'), dg.edge('1', '2')`とか\n",
    "3. (必要があれば)保存, 描画\n",
    "> `dg.view()`で画像ファイル出力.単に`dg`とすればnotebook上で出力.`dg.render(file_path)`とするとパス指定して画像保存できる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.1 (20200629.0846)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"582pt\" height=\"692pt\"\n",
       " viewBox=\"0.00 0.00 582.39 692.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 688)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-688 578.39,-688 578.39,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"48.1\" cy=\"-90\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"48.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">人工知能</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"268.1\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.1\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">語</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;17 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83.44,-77.75C124.98,-64.54 193.42,-42.76 234.29,-29.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.58,-33.02 244.05,-26.65 233.46,-26.35 235.58,-33.02\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"189.1\" cy=\"-90\" rx=\"74.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">じんこうちのう</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;17 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M207.82,-72.41C218.87,-62.62 232.94,-50.15 244.72,-39.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.27,-42.13 252.43,-32.88 242.63,-36.89 247.27,-42.13\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"347.1\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.1\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">AI</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"347.1\" cy=\"-90\" rx=\"65.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">エーアイとは</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M347.1,-143.7C347.1,-135.98 347.1,-126.71 347.1,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.6,-118.1 347.1,-108.1 343.6,-118.1 350.6,-118.1\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;17 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M328.37,-72.41C317.33,-62.62 303.25,-50.15 291.47,-39.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"293.56,-36.89 283.76,-32.88 288.92,-42.13 293.56,-36.89\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"318.1\" cy=\"-666\" rx=\"29.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.1\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\">計算</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"318.1\" cy=\"-594\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.1\" y=\"-590.3\" font-family=\"Times,serif\" font-size=\"14.00\">という</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M318.1,-647.7C318.1,-639.98 318.1,-630.71 318.1,-622.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"321.6,-622.1 318.1,-612.1 314.6,-622.1 321.6,-622.1\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"413.1\" cy=\"-522\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"413.1\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\">道具を</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.78,-578.5C351.54,-568.36 370.1,-554.68 385.35,-543.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"387.46,-546.24 393.44,-537.49 383.31,-540.6 387.46,-546.24\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"413.1\" cy=\"-450\" rx=\"37.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"413.1\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">用いて</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M413.1,-503.7C413.1,-495.98 413.1,-486.71 413.1,-478.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"416.6,-478.1 413.1,-468.1 409.6,-478.1 416.6,-478.1\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"413.1\" cy=\"-594\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"413.1\" y=\"-590.3\" font-family=\"Times,serif\" font-size=\"14.00\">概念と</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M413.1,-575.7C413.1,-567.98 413.1,-558.71 413.1,-550.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"416.6,-550.1 413.1,-540.1 409.6,-550.1 416.6,-550.1\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"508.1\" cy=\"-666\" rx=\"66.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"508.1\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\">コンピュータ</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"508.1\" cy=\"-594\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"508.1\" y=\"-590.3\" font-family=\"Times,serif\" font-size=\"14.00\">という</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M508.1,-647.7C508.1,-639.98 508.1,-630.71 508.1,-622.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"511.6,-622.1 508.1,-612.1 504.6,-622.1 511.6,-622.1\"/>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M488.41,-578.5C474.65,-568.36 456.09,-554.68 440.84,-543.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"442.88,-540.6 432.75,-537.49 438.73,-546.24 442.88,-540.6\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"460.1\" cy=\"-378\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.1\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">研究する</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M424,-432.76C429.73,-424.23 436.88,-413.58 443.3,-404.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"446.26,-405.89 448.93,-395.63 440.45,-401.98 446.26,-405.89\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"460.1\" cy=\"-306\" rx=\"57.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.1\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">計算機科学</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M460.1,-359.7C460.1,-351.98 460.1,-342.71 460.1,-334.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"463.6,-334.1 460.1,-324.1 456.6,-334.1 463.6,-334.1\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"507.1\" cy=\"-450\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.1\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">知能を</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M495.96,-432.41C490.36,-424.08 483.46,-413.8 477.21,-404.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"479.96,-402.31 471.48,-395.96 474.15,-406.21 479.96,-402.31\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"460.1\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.1\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">の</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M460.1,-287.7C460.1,-279.98 460.1,-270.71 460.1,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"463.6,-262.1 460.1,-252.1 456.6,-262.1 463.6,-262.1\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"460.1\" cy=\"-162\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.1\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">一分野を</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M460.1,-215.7C460.1,-207.98 460.1,-198.71 460.1,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"463.6,-190.1 460.1,-180.1 456.6,-190.1 463.6,-190.1\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"460.1\" cy=\"-90\" rx=\"29.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.1\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">指す</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M460.1,-143.7C460.1,-135.98 460.1,-126.71 460.1,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"463.6,-118.1 460.1,-108.1 456.6,-118.1 463.6,-118.1\"/>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M437.33,-78.42C432.34,-76.21 427.07,-73.96 422.1,-72 381.16,-55.84 333.07,-39.73 301.66,-29.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.67,-26.24 292.08,-26.52 300.53,-32.91 302.67,-26.24\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f87f65e1090>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有向グラフをimport\n",
    "from graphviz import Digraph\n",
    "\n",
    "# 文節の文字列を返す関数\n",
    "def get_clause(morphs: list) -> str:\n",
    "    return ''.join([morph.surface for morph in morphs if morph.pos != '記号'])\n",
    "\n",
    "\n",
    "\n",
    "with open('ai.ja.txt.parsed.head') as f:\n",
    "    chunks_list = list(chunk_reader(f))\n",
    "    \n",
    "    # 各EOS毎に処理を繰り返す\n",
    "    for sen_num, chunks in enumerate(chunks_list):\n",
    "        if sen_num == 1:\n",
    "            # 有向グラフオブジェクトを生成\n",
    "            dg = Digraph(format='png')\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # 係元の文節のノード作成\n",
    "                dg.node(str(i), label=get_clause(chunk.morphs))\n",
    "            \n",
    "                # 係先の文節のノード, 辺を作成\n",
    "                if chunk.dst != -1:\n",
    "                    to = chunk.dst\n",
    "                    dg.node(str(chunk.dst), label=get_clause(chunks[chunk.dst].morphs))\n",
    "                    dg.edge(str(i), str(to))\n",
    "\n",
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    ">今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ.  \n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる  \n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． \n",
    "この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである\n",
    "`作り出す　で　は　を`  \n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def josi_pos_extracter(chunks: list, src_num: int) -> str:\n",
    "    for morph in chunks[src_num].morphs:\n",
    "        if morph.pos == '助詞':\n",
    "            return morph.base\n",
    "\n",
    "def verb_pos_extracter(cabocha_parsed_file: str) -> list:\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunks_list = list(chunk_reader(f))\n",
    "    \n",
    "        # 各EOS毎に処理を繰り返す\n",
    "        for chunks in chunks_list:   # for 行 in テキスト全体:\n",
    "            \n",
    "            for chunk in chunks:   # for 文節 in 1行:\n",
    "                \n",
    "                # 現在見ている文節に動詞が含まれているかどうかチェック\n",
    "                if(any([morph for morph in chunk.morphs if morph.pos == '動詞'])):\n",
    "                    # 文節中の最左の動詞\n",
    "                    first_verb = [morph.base for morph in chunk.morphs if morph.pos == '動詞'][0]\n",
    "                \n",
    "                    # この動詞に係る助詞を格納するリスト\n",
    "                    pos_list = [josi_pos_extracter(chunks, src_num) for src_num in chunk.srcs]\n",
    "                    while None in pos_list:\n",
    "                        pos_list.remove(None)\n",
    "                    pos_list = sorted(pos_list)\n",
    "                    \n",
    "                    # ファイルに出力しやすい形式にするために,リストの先頭に係先の動詞を挿入\n",
    "                    pos_list.insert(0, first_verb)\n",
    "                    yield pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['作り出す', 'で', 'は', 'を']]\n"
     ]
    }
   ],
   "source": [
    "# 例文の処理結果と一致するか確認. John.txt.parsedファイルに係受け解析済み例文が格納されている. \n",
    "pprint(list(verb_pos_extracter('John.txt.parsed')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('verb_pos_extract.txt', 'w') as f:\n",
    "    verb_pos_list = list(verb_pos_extracter('ai.ja.txt.parsed'))\n",
    "    for verb_pos in verb_pos_list:\n",
    "        print('\\t'.join(verb_pos), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\r\n",
      "する\tて\tを\r\n",
      "指す\tを\r\n",
      "代わる\tに\tを\r\n",
      "行う\tて\tに\r\n",
      "する\tと\r\n",
      "述べる\tで\tの\tは\r\n",
      "する\tで\tを\r\n",
      "する\tを\r\n",
      "する\tを\r\n"
     ]
    }
   ],
   "source": [
    "!head verb_pos_extract.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- コーパス中で頻出する述語と格パターンの組み合わせ の確認  \n",
    "\n",
    "`uniq -c ファイル名`\n",
    "（指定したファイル内の重複行を取り除き、それぞれの行が何回出現したかを表示する）   \n",
    "\n",
    "`sort -r ファイル名`\n",
    "（指定したファイルの各行を逆順でソート.数値でソートしたいときは`n`つける.`-nr`とか.）  \n",
    "\n",
    "`sort -k キーの列番号 ファイル名`  \n",
    "（指定したファイルの各行を指定のキー番号の値でソート）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  51 する\tを\r\n",
      "  18 する\tが\r\n",
      "  16 する\tに\r\n",
      "  16 する\tと\r\n",
      "  11 する\tに\tを\r\n",
      "  10 する\tは\tを\r\n",
      "   9 する\tで\tを\r\n",
      "   9 する\tが\tに\r\n",
      "   9 よる\tに\r\n",
      "   8 行う\tを\r\n"
     ]
    }
   ],
   "source": [
    "!sort -k 1 verb_pos_extract.txt | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）  \n",
    "\n",
    "`grep -e '探索対象1' -e '探索対象2' ... ファイル名`（指定したファイルないにおいて探索条件に書いたものを探す）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 行う\tを\r\n",
      "   4 なる\tに\tは\r\n",
      "   3 なる\tが\tと\r\n",
      "   2 行う\tは\tを\r\n",
      "   2 なる\tに\r\n",
      "   1 与える\tが\tなど\r\n",
      "   1 与える\tに\tは\tを\r\n",
      "   1 与える\tが\tに\r\n",
      "   1 なる\tから\tが\tて\tで\tと\tば\r\n",
      "   1 なる\tから\tで\tと\r\n"
     ]
    }
   ],
   "source": [
    "!grep -e '^行う' -e '^なる' -e '^与える' verb_pos_extract.txt | sort -k 1 | uniq -c | sort -r | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    ">45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ.  \n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる  \n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．  \n",
    "`作り出す\tで は を\t会議で ジョンマッカーシーは 用語を`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_pos_extracter_46(cabocha_parsed_file: str) -> list:\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunks_list = list(chunk_reader(f))\n",
    "    \n",
    "        # 各EOS毎に処理を繰り返す\n",
    "        for chunks in chunks_list:   # for 行 in テキスト全体:\n",
    "            \n",
    "            for chunk in chunks:   # for 文節 in 1行:\n",
    "                \n",
    "                # 現在見ている文節に動詞が含まれているかどうかチェック\n",
    "                if(any([morph for morph in chunk.morphs if morph.pos == '動詞'])):\n",
    "                    # 文節中の最左の動詞\n",
    "                    first_verb = [morph.base for morph in chunk.morphs if morph.pos == '動詞'][-1]\n",
    "                \n",
    "                    # この動詞に係る助詞を格納するリスト\n",
    "                    pos_list = []\n",
    "                    # この動詞に係る文節を格納するリスト\n",
    "                    clause_list = []\n",
    "                    \n",
    "                    for src_num in chunk.srcs:\n",
    "                        pos_list.append(josi_pos_extracter(chunks, src_num))\n",
    "                        clause_list.append(get_clause(chunks[src_num].morphs))\n",
    "                    while None in pos_list:\n",
    "                        pos_list.remove(None)\n",
    "                    \n",
    "                    # zipを使って二つの配列を同時ソートする\n",
    "                    if not pos_list:\n",
    "                        break\n",
    "                    c = zip(pos_list, clause_list)\n",
    "                    c = sorted(c)\n",
    "                    pos_list, clause_list = zip(*c)\n",
    "                    \n",
    "                    # tupleで返ってきているので,リストに直す\n",
    "                    pos_list = list(pos_list)\n",
    "                    clause_list = list(clause_list)\n",
    "                    \n",
    "                    # 配列を連結\n",
    "                    pos_list.extend(clause_list)\n",
    "                    \n",
    "                    # ファイルに出力しやすい形式にするために,リストの先頭に係先の動詞を挿入\n",
    "                    pos_list.insert(0, first_verb)\n",
    "                    yield pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['作り出す', 'で', 'は', 'を', '会議で', 'ジョンマッカーシーは', '用語を']]\n"
     ]
    }
   ],
   "source": [
    "# 例文の処理結果と一致するか確認. John.txt.parsedファイルに係受け解析済み例文が格納されている. \n",
    "pprint(list(verb_pos_extracter_46('John.txt.parsed')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\t道具を\n",
      "する\tて\tを\t用いて\t知能を\n",
      "指す\tを\t一分野を\n",
      "代わる\tに\tを\t人間に\t知的行動を\n",
      "せる\tて\tに\t代わって\tコンピューターに\n",
      "れる\tと\t研究分野とも\n",
      "いる\tで\tの\tは\t解説で\t次のように\t佐藤理史は\n",
      "する\tで\tを\tコンピュータ上で\t知的能力を\n",
      "する\tを\t推論判断を\n",
      "する\tを\t画像データを\n",
      "する\tて\tを\t解析して\tパターンを\n",
      "ある\tが\tは\t画像認識等が\t応用例は\n",
      "れる\tで\tに\tにより\tダートマス会議で\t1956年に\tジョンマッカーシーにより\n",
      "用いる\tを\t記号処理を\n",
      "する\tと\tを\t主体と\t記述を\n",
      "いる\tで\tでも\t現在では\t意味あいでも\n",
      "れる\tも\t思考ルーチンも\n",
      "ある\tも\tことも\n",
      "する\tを\tカウンセラーを\n",
      "れる\tが\tに\tプログラム\t人工無脳が\n"
     ]
    }
   ],
   "source": [
    "# 出力の確認\n",
    "for verb_pos in list(verb_pos_extracter_46('ai.ja.txt.parsed'))[:20]:\n",
    "    print('\\t'.join(verb_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    ">動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．  \n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）  \n",
    "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．  \n",
    "`学習を行う\tに を\t元に 経験を`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文節に助詞「を」 and サ変接続名詞の両方があるかどうか\n",
    "def sahen_wo_sarcher(chunks: list, src_num: int) -> str:\n",
    "    if len(chunks[src_num].morphs) == 2:\n",
    "        m1, m2 = chunks[src_num].morphs\n",
    "        if m1.pos == '名詞' and m1.pos1 == 'サ変接続' and m2.surface == 'を' and m2.pos == '助詞':\n",
    "            return chunks[src_num]\n",
    "            \n",
    "        \n",
    "def verb_pos_extracter_47(cabocha_parsed_file: str) -> list:\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunks_list = list(chunk_reader(f))\n",
    "    \n",
    "        # 各EOS毎に処理を繰り返す\n",
    "        for chunks in chunks_list:   # for 行 in テキスト全体:\n",
    "            \n",
    "            for chunk in chunks:   # for 文節 in 1行:\n",
    "                \n",
    "                # 現在見ている文節に動詞が含まれているかどうかチェック\n",
    "                if(any([morph for morph in chunk.morphs if morph.pos == '動詞'])):\n",
    "                    # 文節中の最左の動詞\n",
    "                    first_verb = [morph.base for morph in chunk.morphs if morph.pos == '動詞'][0]\n",
    "                \n",
    "                    # この動詞に係る助詞を格納するリスト\n",
    "                    pos_list = []\n",
    "                    # この動詞に係る文節を格納するリスト\n",
    "                    clause_list = []\n",
    "                    \n",
    "                    # 文節に助詞「を」 and サ変接続名詞のペアがいくつあるかカウント\n",
    "                    sahen_wo_count = len([sahen_wo_sarcher(chunks, src_num) for src_num in chunk.srcs])\n",
    "                    \n",
    "                    # 動詞の係もとにサ変名詞 & 「を」があるかどうかを判定\n",
    "                    c = 1\n",
    "                    for src_num in chunk.srcs:\n",
    "                        # 最右のサ変名詞でなければスキップ\n",
    "                        if c != sahen_wo_count:\n",
    "                            c += 1\n",
    "                            continue\n",
    "                        if sahen_wo_sarcher(chunks, src_num):\n",
    "                            sahen_chunk = sahen_wo_sarcher(chunks, src_num)\n",
    "                            pos_list = [josi_pos_extracter(chunks, src_num) for src_num in chunk.srcs]\n",
    "                            clause_list = [get_clause(chunks[src_num].morphs) for src_num in chunk.srcs if chunks[src_num] != sahen_chunk]\n",
    "                            \n",
    "                            while None in pos_list:\n",
    "                                pos_list.remove(None)\n",
    "                     \n",
    "                            pos_list.remove('を')\n",
    "                            \n",
    "                            if not pos_list:\n",
    "                                break\n",
    "                                \n",
    "                            # zipを使って二つの配列を同時ソートする\n",
    "                            c = zip(pos_list, clause_list)\n",
    "                            c = sorted(c)\n",
    "                            pos_list, clause_list = zip(*c)\n",
    "                    \n",
    "                            # アンパックするとtupleで返ってきているので,リストに直す\n",
    "                            pos_list = list(pos_list)\n",
    "                            clause_list = list(clause_list)\n",
    "                            \n",
    "                    \n",
    "                            # 配列を連結\n",
    "                            pos_list.extend(clause_list)\n",
    "                            \n",
    "                            # 述語を整形\n",
    "                            verb_clause = get_clause(sahen_chunk.morphs) + first_verb\n",
    "                            \n",
    "                            # ファイルに出力しやすい形式にするために,リストの先頭に述語を挿入\n",
    "                            pos_list.insert(0, verb_clause)\n",
    "                            yield pos_list\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['学習を行う', 'に', 'を', '経験を', '元に']]\n"
     ]
    }
   ],
   "source": [
    "# 例題の確認\n",
    "pprint(list(verb_pos_extracter_47('q47_parsed.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注目を集める\tが\tその後\n",
      "学習を行う\tに\tを\t経験を\t元に\n",
      "進化を見せる\tて\tにおいて\tは\t敵対的生成ネットワークは\t加えて\t特に\n",
      "開発を行う\tは\tエイダ・ラブレスは\n",
      "意味をする\tに\tデータに\n",
      "研究を進める\tて\t費やして\n",
      "運転をする\tに\t増やし\n",
      "特許をする\tが\tまで\t日本が\t2018年までに\n",
      "運転をする\tて\tに\t基づいて\t柔軟に\n",
      "注目を集める\tから\tは\tことから\tファジィは\n",
      "研究を続ける\tが\tて\tジェフホーキンスが\t向けて\n",
      "注目を集める\tに\t急速に\n",
      "投資を行う\tで\tに\t民間企業主導で\t全世界的に\n",
      "探索を行う\tで\t実装し\n",
      "研究を行う\tは\tGoogleは\n",
      "投資をする\tは\tまで\t韓国は\t2022年までに\n",
      "反乱を起こす\tて\tに対して\t於いて\t人間に対して\n",
      "判断を介す\tから\t観点から\n",
      "禁止を求める\tが\tに\tヒューマン・ライツ・ウォッチが\t4月には\n",
      "追及を受ける\tて\tで\tで\tと\tとともに\t暴露されており\t公聴会では\t整合性で\t拒否すると\tとともに\n"
     ]
    }
   ],
   "source": [
    "# 出力の確認\n",
    "for verb_pos in list(verb_pos_extracter_47('ai.ja.txt.parsed'))[:20]:\n",
    "    print('\\t'.join(verb_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    ">文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．  \n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する  \n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる  \n",
    "`ジョンマッカーシーは -> 作り出した  \n",
    "AIに関する -> 最初の -> 会議で -> 作り出した  \n",
    "最初の -> 会議で -> 作り出した  \n",
    "会議で -> 作り出した  \n",
    "人工知能という -> 用語を -> 作り出した  \n",
    "用語を -> 作り出した`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_pos_extracter(chunk) -> str:\n",
    "    for morph in chunk.morphs:\n",
    "        if morph.pos == '名詞':\n",
    "            return morph\n",
    "\n",
    "def noun_to_end(cabocha_parsed_file: str) -> str:\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunks_list = list(chunk_reader(f))\n",
    "        # 各EOS毎に処理を繰り返す\n",
    "        for chunks in chunks_list:   # for 行 in テキスト全体:\n",
    "            for chunk in chunks:   # for 文節 in 1行:\n",
    "                if noun_pos_extracter(chunk):\n",
    "                    while chunk.dst != -1:\n",
    "                        print(get_clause(chunk.morphs), ' -> ', end='')\n",
    "                        chunk = chunks[chunk.dst]\n",
    "                    if chunk.dst == -1:\n",
    "                        print(get_clause(chunk.morphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョンマッカーシーは  -> 作り出した\n",
      "AIに関する  -> 最初の  -> 会議で  -> 作り出した\n",
      "最初の  -> 会議で  -> 作り出した\n",
      "会議で  -> 作り出した\n",
      "人工知能という  -> 用語を  -> 作り出した\n",
      "用語を  -> 作り出した\n"
     ]
    }
   ],
   "source": [
    "noun_to_end('John.txt.parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    ">文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj\n",
    "（i<j\n",
    "）のとき，係り受けパスは以下の仕様を満たすものとする．  \n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する  \n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．    \n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示  \n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる  \n",
    "`\n",
    "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
    "Xは | Yの -> 会議で | 作り出した\n",
    "Xは | Yで | 作り出した\n",
    "Xは | Yという -> 用語を | 作り出した\n",
    "Xは | Yを | 作り出した\n",
    "Xに関する -> Yの\n",
    "Xに関する -> 最初の -> Yで\n",
    "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
    "Xの -> Yで\n",
    "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xの -> 会議で | Yを | 作り出した\n",
    "Xで | Yという -> 用語を | 作り出した\n",
    "Xで | Yを | 作り出した\n",
    "Xという -> Yを`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文節を受け取り, その名詞の部分を文字列subで置き換えて出力\n",
    "def get_sub_clause(morphs: list, sub: str) -> str:\n",
    "    return sub + ''.join([morph.surface for morph in morphs if morph.pos != '記号' and morph.pos != '名詞'])\n",
    "\n",
    "# その文節に名詞が含まれるかどうかを判定\n",
    "def noun_pos_extracter(chunk) -> str:\n",
    "    for morph in chunk.morphs:\n",
    "        if morph.pos == '名詞':\n",
    "            return True\n",
    "        \n",
    "# 1行分のchunkと文節iを受け取り, iとペアになるjのリストを返す.\n",
    "def get_i_j_pairs(chunks: list, src_num: int) -> list:\n",
    "    assert noun_pos_extracter(chunks[src_num]), 'This chunk does not include noun clause'\n",
    "    j_s = []\n",
    "    for index in range(src_num+1, len(chunks)):\n",
    "        if noun_pos_extracter(chunks[index]):\n",
    "            j_s.append(chunks[index])\n",
    "        \n",
    "    return j_s\n",
    "\n",
    "# 文節i, jを受け取り,根までのそれぞれのパスにおいて共通している文節のリストを返す\n",
    "def common_path(chunks: list, chunk_i, chunk_j) -> list:\n",
    "    i_path = set()\n",
    "    original_i = chunk_i\n",
    "    while chunk_i.dst != -1:\n",
    "        i_path.add(chunk_i)\n",
    "        chunk_i = chunks[chunk_i.dst]\n",
    "        \n",
    "    i_path.remove(original_i)   # i自身は取り出す\n",
    "        \n",
    "    j_path = set()\n",
    "    original_j = chunk_j\n",
    "    while chunk_j.dst != -1:\n",
    "        j_path.add(chunk_j)\n",
    "        chunk_j = chunks[chunk_j.dst]\n",
    "        \n",
    "    j_path.remove(original_j)   # j自身は取り出す\n",
    "    \n",
    "    common_path = list(i_path & j_path)\n",
    "    \n",
    "    return common_path\n",
    "        \n",
    "\n",
    "# ある文節から根までへ,幾つの文節を経るかをカウント\n",
    "def path_length(chunks: list, chunk) -> int:\n",
    "    length = 0\n",
    "    while chunk.dst != -1:\n",
    "        chunk = chunks[chunk.dst]\n",
    "        length += 1 \n",
    "        \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_noun_to_end(cabocha_parsed_file: str) -> str:\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunks_list = list(chunk_reader(f))\n",
    "        # 各EOS毎に処理を繰り返す\n",
    "        for chunks in chunks_list:   # for 行 in テキスト全体:\n",
    "            \n",
    "            for index, i_chunk in enumerate(chunks):   # for 文節 in 1行:\n",
    "                \n",
    "                if noun_pos_extracter(i_chunk):  # 文節i\n",
    "                    \n",
    "                    j_s = get_i_j_pairs(chunks, index)   # ペアとなる文節j\n",
    "                    \n",
    "                    for j_chunk in j_s:\n",
    "                        common_chunks = common_path(chunks, i_chunk, j_chunk)\n",
    "                        print(f'common_chunksは{common_chunks}')\n",
    "                        # i->根までの途中にjがある場合(「|」が必要ない)\n",
    "                        print(f'len(common_chunks) = {len(common_chunks)}, path_length = {path_length(chunks, j_chunk)}')\n",
    "                        \n",
    "                        if len(common_chunks) == path_length(chunks, j_chunk):\n",
    "                            print('i->根までの途中にjがあるパターン\\n')\n",
    "                            path_chunk = i_chunk\n",
    "                            \n",
    "                            while path_chunk.dst != -1:\n",
    "                                if path_chunk == i_chunk:   # 文節iを出力\n",
    "                                    print(get_sub_clause(i_chunk.morphs, 'X'), ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                                        \n",
    "                                elif path_chunk == j_chunk:   # 文節jを出力\n",
    "                                    print(get_sub_clause(j_chunk.morphs, 'Y'), ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                                    \n",
    "                                else:   # それ以外の文節を出力\n",
    "                                    print(get_clause(path_chunk.morphs), ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                            if path_chunk.dst == -1:\n",
    "                                print(get_clause(path_chunk.morphs))\n",
    "                                print('=========')\n",
    "                        \n",
    "                        # i->根までの途中にjがない場合(「|」が必要)\n",
    "                        else:\n",
    "                            print('i->根までの途中にjがないパターン\\n')\n",
    "                            path_chunk = i_chunk\n",
    "                            \n",
    "                            while path_chunk.dst != -1:\n",
    "                                if path_chunk == i_chunk:   # 文節iを出力\n",
    "                                    print(get_sub_clause(i_chunk.morphs, 'X'), ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                                        \n",
    "                                elif path_chunk == j_chunk:   # 文節jを出力\n",
    "                                    print(get_sub_clause(j_chunk.morphs, 'Y'), ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                                        \n",
    "                                elif path_chunk in common_chunks:   # 文節kを出力\n",
    "                                    print('|', ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                                        \n",
    "                                else:   # それ以外の文節を出力\n",
    "                                    print(get_clause(path_chunk.morphs), ' -> ', end='')\n",
    "                                    path_chunk = chunks[path_chunk.dst]\n",
    "                                    \n",
    "                            if path_chunk.dst == -1:\n",
    "                                print(get_clause(path_chunk.morphs))\n",
    "                                print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 3\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xは  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 2\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xは  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xは  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 2\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xは  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xは  -> 作り出した\n",
      "=========\n",
      "common_chunksは[Chunk(morphs=[Morph(surface='会議', base='会議', pos='名詞', pos1='サ変接続'), Morph(surface='で', base='で', pos='助詞', pos1='格助詞')], dst=6, srcs=[2])]\n",
      "len(common_chunks) = 1, path_length = 2\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xに関する  -> Yの  -> |  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xに関する  -> 最初の  -> Yで  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 2\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xに関する  -> 最初の  -> 会議で  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xに関する  -> 最初の  -> 会議で  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xの  -> Yで  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 2\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xの  -> 会議で  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xの  -> 会議で  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 2\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xで  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xで  -> 作り出した\n",
      "=========\n",
      "common_chunksは[]\n",
      "len(common_chunks) = 0, path_length = 1\n",
      "i->根までの途中にjがないパターン\n",
      "\n",
      "Xという  -> Yを  -> 作り出した\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "pair_noun_to_end('John.txt.parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解き直し編〜Classのメソッドを拡充して解く(47~49)\n",
    "せっかくクラスを使っているので諸々の関数をメソッドとして用意してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Chunk(morphs={repr(self.morphs)}, dst={repr(self.dst)}, srcs={repr(self.srcs)})\"\n",
    "    \n",
    "    # 文節の文全体の文字列を取得\n",
    "    def get_surface(self) -> str:\n",
    "        return ''.join(morph.surface for morph in self.morphs if morph.pos != '記号')\n",
    "    \"\"\"\n",
    "    # 文節の指定のposを持つ形態素の表層形を文字列stringに置き換えた文字列を取得\n",
    "    def get_surface_swap(self, pos, string) -> str:\n",
    "        tmp_morphs = self.morphs\n",
    "        for tmp_morph in tmp_morphs:\n",
    "            if tmp_morph.pos == pos:\n",
    "                tmp_morph.surface = string\n",
    "                \n",
    "        return ''.join(morph.surface for morph in tmp_morphs if morph.pos != '記号')\n",
    "    \"\"\"\n",
    "    # 文節の中に引数のposが含まれているかどうか判定\n",
    "    def include_pos(self, pos) -> bool:\n",
    "        return any(morph.pos == pos for morph in self.morphs)\n",
    "    \n",
    "    # 文節の中に含まれているpos(1番はじめに見つかったもの=最左)を返す\n",
    "    def get_pos_left(self, pos):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return morph\n",
    "            \n",
    "    # 文節の中に含まれているpos(1番はじめに見つかったもの=最右)を返す        \n",
    "    def get_pos_right(self, pos):\n",
    "        for morph in self.morphs[::-1]:\n",
    "            if morph.pos == pos:\n",
    "                return morph\n",
    "    \n",
    "    # 文節の中に引数のpos1が含まれているかどうか判定\n",
    "    def include_pos1(self, pos1) -> bool:\n",
    "        return any(morph.pos1 == pos1 for morph in self.morphs)\n",
    "    \n",
    "    # 文節の中に含まれているpos1(1番はじめに見つかったもの=最左)を返す\n",
    "    def get_pos1_left(self, pos1):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos1 == pos1:\n",
    "                return morph\n",
    "            \n",
    "    # 文節の中に含まれているpos1(1番はじめに見つかったもの=最右)を返す        \n",
    "    def get_pos1_right(self, pos1):\n",
    "        for morph in self.morphs[::-1]:\n",
    "            if morph.pos1 == pos1:\n",
    "                return morph\n",
    "            \n",
    "    # 文節中に含まれている,指定したposを持つ全てのmorphを返す\n",
    "    def get_pos_all(self, pos):\n",
    "        return [morph for morph in self.morphs if morph.pos == pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47\n",
    "\n",
    "def re_sahen_wo_getter(cabocha_parsed_file :str):\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunk_list = list(chunk_reader(f))\n",
    "        for chunks in chunk_list:   # 1行 in text\n",
    "            for chunk in chunks:   # 1文節 in 1行\n",
    "                \n",
    "                # ===== 情報取得 =====\n",
    "                if chunk.include_pos('動詞'):\n",
    "                    left_verb = chunk.get_pos_left('動詞')   # 最左の動詞を取得\n",
    "                    src_chunk_list = []\n",
    "                    sahen_wo = False   # 初期化するならsrc_chunk_listに入っていないような値使うと良い.-1とか\n",
    "                    for src in chunk.srcs:   # 動詞に係る各文節について\n",
    "                        src_chunk = chunks[src]\n",
    "                        if src_chunk.include_pos1('サ変接続') and src_chunk.include_pos('助詞'):\n",
    "                            if src_chunk.get_pos_right('助詞').base == 'を':# 「サ変接続+を」を含む文節\n",
    "                                sahen_wo = src_chunk\n",
    "                        if src_chunk.include_pos('助詞'):   # 「助詞」を含む文節\n",
    "                            src_chunk_list.append(src_chunk)\n",
    "                    \n",
    "                    # ===== 整形 =====\n",
    "                    # リストから「サ変接続+を」自身を除外 & ソート\n",
    "                    if not sahen_wo in src_chunk_list:\n",
    "                        continue\n",
    "                    src_chunk_list.remove(sahen_wo)\n",
    "                    src_chunk_list.sort(key=lambda x: x.morphs[-1].base)  # 助詞はmorphsの最後に格納されているものでソート\n",
    "                    \n",
    "                    surface_list = [chunk.get_surface() for chunk in src_chunk_list]\n",
    "                    josi_list = [morph.base for chunk in src_chunk_list for morph in chunk.morphs if morph.pos == '助詞']\n",
    "                \n",
    "                    # ===== 出力 =====(「サ変接続+を」がある場合のみ出力\n",
    "                    if sahen_wo:\n",
    "                        print(sahen_wo.get_surface(), end = '')\n",
    "                        print(left_verb.base, end='\\t')\n",
    "                        print(' '.join(josi_list), end = '\\t')\n",
    "                        print(' '.join(surface_list))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習を行う\tに を\t元に 経験を\n"
     ]
    }
   ],
   "source": [
    "re_sahen_wo_getter('q47_parsed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "知的行動を代わる\tに\t人間に\n",
      "推論判断をする\t\t\n",
      "記号処理を用いる\t\t\n",
      "記述をする\tと\t主体と\n",
      "注目を集める\tが\tサポートベクターマシンが\n",
      "学習を行う\tに を\t元に 経験を\n",
      "流行を超える\t\t\n",
      "学習を繰り返す\t\t\n",
      "統計的学習をする\tで は を に を通して\tACT-Rでは 推論ルールを 元に 生成規則を通して\n",
      "進化を見せる\tは て において\t敵対的生成ネットワークは 加えて 生成技術において\n",
      "コンテンツ生成を行う\t\t\n",
      "機械式計算機をする\tは\tブレーズ・パスカルは\n",
      "開発を行う\tは\tエイダ・ラブレスは\n",
      "プログラミング言語をする\tは\t彼はまた\n",
      "テストをする\t\t\n",
      "来談者中心療法を行う\t\t\n",
      "プログラミング言語をする\tは\tアランカルメラウアーは\n",
      "バックギャモン専用コンピュータTDギャモンをする\tに は\t1992年に IBMは\n",
      "投資全額を上回る\tが\tコストが\n",
      "意味付けをする\tに対して から\t非構造化データに対して ここから\n",
      "処理を行う\t\t\n",
      "知的処理を行う\tにより に\tティム・バーナーズリーにより Webに\n",
      "意味をする\tに\tデータに\n",
      "知的処理を行う\tて に\t付加して コンピュータに\n",
      "研究を進める\tて\t費やして\n",
      "命令をする\tで\t機構で\n",
      "運転をする\tに\t元に\n",
      "特許をする\tが まで に\t日本が 2018年までに\n",
      "研究をする\t\t\n",
      "運転をする\tて に\t基づいて 柔軟に\n",
      "注目を集める\tから は\tことから ファジィは\n",
      "ニューロファジィ制御をする\t\t\n",
      "成功を受ける\t\t\n",
      "知的制御を用いる\tて も\t受けて 他社も\n",
      "開発工数を抑える\t\t\n",
      "制御をする\tから\t少なさから\n",
      "知的制御をする\t\t\n",
      "表現するをする\t\t\n",
      "進歩を担う\t\t\n",
      "精度改善を果たす\tが で に\tチームが 画像処理コンテストで 2012年に\n",
      "専用プログラムを使う\t\t\n",
      "研究を続ける\tが て\tジェフホーキンスが 向けて\n",
      "行動型システムを用いる\tは は\tものではなく これは\n",
      "関連性を導き出す\t\t\n",
      "ワンショット学習をする\tが\tデータが\n",
      "認識能力を持つ\t\t\n",
      "記号接地問題(シンボルグラウンディング問題)をする\tに は\t8月には\n",
      "注目を集める\tに\t急速に\n",
      "普及を受ける\t\t\n",
      "機械学習を組み合わせる\t\t\n",
      "投資を行う\tで に\t民間企業主導で 全世界的に\n",
      "探索を行う\tで\t無報酬で\n",
      "推論をする\tて\t経て\n",
      "共同研究を始める\tとも\tマックスプランク研究所とも\n",
      "研究を行う\tは\tGoogleは\n",
      "研究開発をする\tで で は\t官民一体で 中国では\n",
      "実験をする\t\t\n",
      "研究開発をする\tで\t日本で\n",
      "投資をする\tは まで に\t韓国は 2022年までに\n",
      "深層学習をする\t\t\n",
      "脳シミュレーションを行う\t\t\n",
      "反乱を起こす\tて に対して\t於いて 人間に対して\n",
      "弾圧を併せ持つ\t\t\n",
      "監視を行う\tに まで\t人工知能に 歩行者まで\n",
      "法的手続きを経る\tを\tウイグル族を\n",
      "監視社会化を恐れる\t\t\n",
      "監視カメラをする\t\t\n",
      "AI監視技術をする\tに は\t世界各国に 中国は\n",
      "差別を認める\t\t\n",
      "研究をする\t\t\n",
      "展開を変える\t\t\n",
      "戦争をする\t\t\n",
      "ファジィ制御をする\tは\tAIプログラムは\n",
      "判断を介す\tから\t観点から\n",
      "開発禁止令を出す\tに\t2012年に\n",
      "禁止を求める\tが に は\tヒューマン・ライツ・ウォッチが 4月には\n",
      "運用をめぐる\t\t\n",
      "開発競争を行う\tは をめぐって\t米国中国ロシアは 軍事利用をめぐって\n",
      "記録をする\t\t\n",
      "自律無人艇を使う\t\t\n",
      "試験を行う\t\t\n",
      "メイヴン計画を行う\t\t\n",
      "追及を受ける\tて で は とともに で と\t暴露されており 公聴会では とともに 整合性で 拒否すると\n",
      "共同研究をする\tが\tMicrosoftが\n",
      "監視国家をする\tが によって\t中国が AIによって\n",
      "無人攻撃機をする\tで に\t中東で 大量に\n",
      "反科学反マイノリティ・地球温暖化懐疑論等をする\t\t\n",
      "解任をする\tて は\t含まれており Google社員らは\n",
      "解散をする\tで が は\t理由で 倫理委員会が Googleは\n",
      "霊的存在を見いだす\tに\tものに\n",
      "道)」をする\tは\tアンソニーレバンドウスキーは\n",
      "実現をする\t\t\n",
      "話をする\tば は\tよれば 哲学者は\n",
      "意思疎通を行う\t\t\n",
      "勘違いをする\t\t\n",
      "議論を行う\tで は まで\t対談で 須藤は これまで\n"
     ]
    }
   ],
   "source": [
    "re_sahen_wo_getter('ai.ja.txt.parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48\n",
    "\n",
    "def re_noun_to_end(cabocha_parsed_file: str) -> str:\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunks_list = list(chunk_reader(f))\n",
    "        # 各EOS毎に処理を繰り返す\n",
    "        for chunks in chunks_list:   # for 行 in テキスト全体:\n",
    "            for chunk in chunks:   # for 文節 in 1行:\n",
    "                if chunk.include_pos('名詞'):\n",
    "                    while chunk.dst != -1:   # 係先を辿る\n",
    "                        print(get_clause(chunk.morphs), ' -> ', end='')\n",
    "                        chunk = chunks[chunk.dst]\n",
    "                    if chunk.dst == -1:   # 根に到達\n",
    "                        print(get_clause(chunk.morphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョンマッカーシーは  -> 作り出した\n",
      "AIに関する  -> 最初の  -> 会議で  -> 作り出した\n",
      "最初の  -> 会議で  -> 作り出した\n",
      "会議で  -> 作り出した\n",
      "人工知能という  -> 用語を  -> 作り出した\n",
      "用語を  -> 作り出した\n"
     ]
    }
   ],
   "source": [
    "re_noun_to_end('John.txt.parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49\n",
    "from itertools import combinations\n",
    "import copy\n",
    "\n",
    "# 1文分のchunkのリストを受け取り,全ての名詞句文節のペアを返す.\n",
    "def get_i_j_pair(chunks:list) -> list:\n",
    "    all_nouns = [chunk for chunk in chunks if chunk.include_pos('名詞')]\n",
    "    if len(all_nouns) <= 1:   # ペアが作れなかった場合\n",
    "        return None\n",
    "    \n",
    "    return list(combinations(all_nouns, 2))\n",
    "\n",
    "# ある文節が根に到達するまでに辿る全ての文節を取得\n",
    "def get_path_chunk(chunks, chunk)->list:\n",
    "    path_chunks = []\n",
    "    while chunk.dst != -1:\n",
    "        path_chunks.append(chunks[chunk.dst])\n",
    "        chunk = chunks[chunk.dst]\n",
    "    if chunk.dst == -1:\n",
    "        path_chunks.append(chunks[chunk.dst])\n",
    "        \n",
    "    return path_chunks\n",
    "\n",
    "# 文節の名詞句を引数の文字列に入れ替える\n",
    "def get_surface_swap(chunk, string) -> str:\n",
    "    noun_string = ''.join([morph.surface for morph in chunk.morphs if morph.pos == '名詞'])\n",
    "    return chunk.get_surface().replace(noun_string, string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_noun_pair_extracter(cabocha_parsed_file: str):\n",
    "    with open(cabocha_parsed_file) as f:\n",
    "        chunk_list = list(chunk_reader(f))\n",
    "        \n",
    "        for chunks in chunk_list:   # 1文分の処理\n",
    "            i_j_pairs = get_i_j_pair(chunks)\n",
    "            \n",
    "            if not i_j_pairs:   # 名詞句のペアが作れなかった文に関しては処理しない. (i_j_pair = None)\n",
    "                continue\n",
    "\n",
    "            for one_pair in i_j_pairs:   # それぞれのペアについて見ていく\n",
    "                i_chunk, j_chunk = one_pair\n",
    "                i_path_chunk = get_path_chunk(chunks, i_chunk)\n",
    "\n",
    "                # i->j->根の場合\n",
    "                if j_chunk in i_path_chunk:\n",
    "                    print(get_surface_swap(i_chunk, 'X'), end = ' -> ')\n",
    "                    for path_chunk in i_path_chunk:\n",
    "\n",
    "                        if path_chunk == j_chunk:\n",
    "                            print(get_surface_swap(j_chunk, 'Y'))\n",
    "                            break\n",
    "                            \n",
    "                        elif path_chunk == chunks[-1]:\n",
    "                            print(path_chunk.get_surface())\n",
    "                            break\n",
    "                            \n",
    "                        else:\n",
    "                            print(path_chunk.get_surface(), end=' -> ')\n",
    "                            \n",
    "                # iとjが途中で合流->根の場合\n",
    "                else:\n",
    "                    j_path_chunk = get_path_chunk(chunks, j_chunk)\n",
    "                    # 文節iとjで共通している文節を取得\n",
    "                    common_chunk = list(set(i_path_chunk) & set(j_path_chunk))\n",
    "                    # 文節iとjが合流する文節\n",
    "                    session_point = common_chunk[0]\n",
    "                    #print(f'今回のセッションポイントは【{session_point.get_surface()}】です')\n",
    "                    \n",
    "                    print(get_surface_swap(i_chunk, 'X'), end = '')\n",
    "                    for path_chunk in i_path_chunk:\n",
    "                            \n",
    "                        if path_chunk == session_point:\n",
    "                            print(' | ', end='')\n",
    "                            print(get_surface_swap(j_chunk, 'Y'), end='')\n",
    "\n",
    "                            session_index = j_path_chunk.index(session_point)\n",
    "                            \n",
    "                            # jから合流文節まで辿る\n",
    "                            for index in range(session_index+1): \n",
    "                                if j_path_chunk[index] == session_point:\n",
    "                                    print(' | ', end='')\n",
    "                                    print(session_point.get_surface())\n",
    "                                    break\n",
    "                                else:\n",
    "                                    print(' -> ', end='')\n",
    "                                    print(j_path_chunk[index].get_surface(), end='')\n",
    "                            break\n",
    "                        else:\n",
    "                            print(' -> ', end='')\n",
    "                            print(path_chunk.get_surface(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "Xは | Yの -> 会議で | 作り出した\n",
      "Xは | Yで | 作り出した\n",
      "Xは | Yという -> 用語を | 作り出した\n",
      "Xは | Yを | 作り出した\n",
      "Xに関する -> Yの\n",
      "Xに関する -> 最初の -> Yで\n",
      "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "Xの -> Yで\n",
      "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xの -> 会議で | Yを | 作り出した\n",
      "Xで | Yという -> 用語を | 作り出した\n",
      "Xで | Yを | 作り出した\n",
      "Xという -> Yを\n"
     ]
    }
   ],
   "source": [
    "re_noun_pair_extracter('John.txt.parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
